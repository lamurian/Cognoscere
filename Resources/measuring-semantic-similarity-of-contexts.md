---
author: Lam
date: 2024-11-29T20:22:09+01:00
title: Measuring semantic similarity of contexts
tags:
- LLM
- NLP
- embedding
---

For nuanced understanding, use transformer-based models like BERT or GPT:
- Tokenize the phrases.
- Compare their contextualized embeddings.

Advantages:
- Context-sensitive and highly robust.

Tools:
- Hugging Face Transformers: BERT, GPT, RoBERTa.

# Relevant notes

- [measuring-phrase-and-word-similarity](Resources/measuring-phrase-and-word-similarity.md) 
- [measuring-semantic-similarity-of-contexts-with-SBERT-in-python](Resources/measuring-semantic-similarity-of-contexts-with-SBERT-in-python.md) 
- [measuring-semantic-similarity-of-contexts-with-SBERT-in-R](Resources/measuring-semantic-similarity-of-contexts-with-SBERT-in-R.md) 
